------------------------ Deutsche Version unten--------------------------------------------------


### 1. 分类交叉熵损失函数

如何写一个**分类交叉熵损失函数（Categorical Cross Entropy Loss Function）**。


这个损失函数用来衡量预测概率和真实标签之间的差异。


在机器学习中，特别是分类任务中，交叉熵损失很常用，它帮助模型在每次训练中减少误差，使得预测的概率更接近真实标签。

公式如下：

**$$
l = \sum_{n=0}^{N_c} y_{\text{true}}^i \log(y_{\text{pred}}^i)
$$**

### 2. 公式解读

我们来分步骤解释公式中每个部分的意义：


- **l**: 这是最终的交叉熵损失值，对于单个数据点计算。
- **$\sum_{n=0}^{N_c}$**: 表示对所有类别进行求和，范围从 0 到 $N_c$，其中 $N_c$ 是类别的总数。
-
- 例如，在手写数字识别的任务中，我们有 10 个类别（数字 0 到 9），所以 $N_c = 10$。
- 
- **$y_{\text{true}}^i$**: 这是真实标签在 One-Hot 编码形式下的第 $i$ 个分量。对于 One-Hot 编码的标签，只有一个位置是 1，其他位置都是 0。
- **$\log(y_{\text{pred}}^i)$**: 这是模型预测的第 $i$ 个类别的概率的对数值，**$y_{\text{pred}}^i$** 是模型给出的第 $i$ 类的概率。取对数的原因是为了在交叉熵中衡量差异。


### 3. 交叉熵损失的目的

交叉熵损失是用来衡量模型预测概率和真实分布之间的差异。
简单来说，交叉熵损失越小，表示模型的预测越接近真实值。
因此，我们希望通过训练来不断降低交叉熵损失的值。

### 4. 代码实现

一步步实现这个交叉熵损失函数：

1. **输入参数**：
   - `params`：模型的参数，例如权重和偏置。
   - `x`：输入数据。
   - `y_true`：真实标签，以 One-Hot 编码表示。

2. **前向传播**：
   使用模型的参数和输入数据，得到预测的概率分布。这个概率分布通常使用 **softmax** 函数进行归一化，使得所有类别的预测值加起来等于 1。

3. **计算损失**：
   - 首先对每个类别计算 $$( y_{\text{true}}^i \times \log(y_{\text{pred}}^i) \)$$。因为 One-Hot 编码的特性，只有真实标签对应的位置会有非零值。
   - 对所有类别进行求和，得到每个样本的损失值。
   - 最后计算所有样本的平均损失，得到最终的交叉熵损失。

### 5. 举例说明

假设我们有三个类别（0, 1, 2），并且我们有一个样本，它的真实标签是类别 `1`，所以它的 One-Hot 编码是 `[0, 1, 0]`。

如果模型预测的概率为 `[0.1, 0.7, 0.2]`，我们可以通过公式来计算交叉熵损失：

$$
l = 0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2) = \log(0.7)
$$

损失的值是 $$(\log(0.7)\)$$（取负号以后），表示模型对于这个样本的预测偏差。损失值越小，表示模型的预测越接近真实值。

### 6. 总结

- 交叉熵损失函数用于衡量模型预测值和真实值的差异。
- 它的公式通过求和计算每个类别上的误差，衡量模型对每个样本预测的准确性。
- 最终目的是通过不断减少交叉熵损失，使得模型的预测越来越准确。

以上为交叉熵损失函数的原理和实现步骤！







### 1. Kategorischer Kreuzentropie-Verlustfunktion

Wie man eine **kategorische Kreuzentropie-Verlustfunktion (Categorical Cross Entropy Loss Function)** schreibt.

Diese Verlustfunktion wird verwendet, um die Differenz zwischen der vorhergesagten Wahrscheinlichkeit und den tatsächlichen Labels zu messen.

In der maschinellen Lernens, insbesondere bei Klassifikationsaufgaben, ist der Kreuzentropie-Verlust sehr gebräuchlich. Er hilft dem Modell, den Fehler bei jedem Training zu verringern, sodass die vorhergesagte Wahrscheinlichkeit immer näher an das tatsächliche Label herankommt.

Die Formel lautet:

**$$
l = \sum_{n=0}^{N_c} y_{\text{true}}^i \log(y_{\text{pred}}^i)
$$**

### 2. Erklärung der Formel

Lassen Sie uns die Bedeutung jedes Teils der Formel schrittweise erklären:

- **l**: Dies ist der endgültige Kreuzentropie-Verlustwert, der für einen einzelnen Datenpunkt berechnet wird.
- **$\sum_{n=0}^{N_c}$**: Steht für die Summe über alle Klassen, von 0 bis $N_c$, wobei $N_c$ die Gesamtanzahl der Klassen darstellt.
- Zum Beispiel gibt es bei der Erkennung handgeschriebener Zahlen 10 Klassen (die Zahlen 0 bis 9), also ist $N_c = 10$.
- **$y_{\text{true}}^i$**: Dies ist die $i$-te Komponente des tatsächlichen Labels im One-Hot-Codierungsformat. Bei One-Hot-Codierung hat nur eine Position den Wert 1, alle anderen Positionen sind 0.
- **$\log(y_{\text{pred}}^i)$**: Dies ist der Logarithmus der Wahrscheinlichkeit der $i$-ten Klasse, die vom Modell vorhergesagt wurde, wobei **$y_{\text{pred}}^i$** die vom Modell gegebene Wahrscheinlichkeit der $i$-ten Klasse ist. Der Logarithmus wird genommen, um in der Kreuzentropie die Unterschiede zu messen.

### 3. Ziel des Kreuzentropie-Verlustes

Der Kreuzentropie-Verlust wird verwendet, um die Abweichung zwischen der vorhergesagten Wahrscheinlichkeit und der tatsächlichen Verteilung zu messen.
Einfach gesagt: Je kleiner der Kreuzentropie-Verlust ist, desto näher liegt die Vorhersage des Modells am tatsächlichen Wert.
Daher wollen wir durch das Training den Kreuzentropie-Verlustwert kontinuierlich verringern.

### 4. Implementierung des Codes

Schrittweises Implementieren dieser Kreuzentropie-Verlustfunktion:

1. **Eingabeparameter**:
   - `params`: Die Parameter des Modells, wie z. B. Gewichtungen und Bias.
   - `x`: Eingabedaten.
   - `y_true`: Das tatsächliche Label, dargestellt in One-Hot-Codierung.

2. **Vorwärtsdurchlauf**:
   Verwenden Sie die Modellparameter und die Eingabedaten, um die Wahrscheinlichkeitsverteilung vorherzusagen. Diese Wahrscheinlichkeitsverteilung wird normalerweise mit der **Softmax**-Funktion normalisiert, sodass die Summe der Vorhersagewerte aller Klassen gleich 1 beträgt.

3. **Berechnung des Verlustes**:
   - Berechnen Sie zunächst für jede Klasse $$( y_{\text{true}}^i \times \log(y_{\text{pred}}^i) \)$$. Aufgrund der Eigenschaften der One-Hot-Codierung wird nur die Position des tatsächlichen Labels einen nicht-null Wert haben.
   - Summieren Sie die Ergebnisse über alle Klassen, um den Verlustwert für jede Stichprobe zu erhalten.
   - Berechnen Sie schließlich den durchschnittlichen Verlust für alle Stichproben, um den endgültigen Kreuzentropie-Verlust zu erhalten.

### 5. Beispiel zur Veranschaulichung

Angenommen, wir haben drei Klassen (0, 1, 2) und eine Stichprobe, deren tatsächliches Label Klasse `1` ist, sodass ihre One-Hot-Codierung `[0, 1, 0]` lautet.

Wenn die Vorhersage des Modells `[0, 1, 0]` lautet, können wir den Kreuzentropie-Verlust mithilfe der Formel berechnen:

$$
l = 0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2) = \log(0.7)
$$

Der Verlustwert beträgt $$(\log(0.7))$$ (nach der negativen Umkehrung), was die Abweichung der Modellvorhersage für diese Stichprobe angibt. Je kleiner der Verlustwert, desto näher liegt die Vorhersage des Modells am tatsächlichen Wert.

### 6. Zusammenfassung

- Die Kreuzentropie-Verlustfunktion wird verwendet, um die Abweichung zwischen den vorhergesagten Werten und den tatsächlichen Werten des Modells zu messen.
- Ihre Formel berechnet den Fehler für jede Klasse und misst, wie genau das Modell für jede Stichprobe vorhersagt.
- Das Endziel ist es, durch ständige Verringerung des Kreuzentropie-Verlustes die Genauigkeit der Modellvorhersagen zu erhöhen.

Dies sind die Grundlagen und Implementierungsschritte für die Kreuzentropie-Verlustfunktion!




